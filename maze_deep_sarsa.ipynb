{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import EnvironmentGoogleSnake\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessEnv(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env=env)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return torch.tensor(obs).unsqueeze(dim = 0).float()\n",
    "    \n",
    "    def step(self, action: torch.Tensor):\n",
    "        action = action.item()\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = torch.tensor(next_state).unsqueeze(dim = 0).float()\n",
    "        reward = torch.tensor(reward).view(1, -1).float()\n",
    "        done = torch.tensor(done).view(1, -1)\n",
    "        return next_state, reward, done, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# env = EnvironmentGoogleSnake()\n",
    "# env = PreProcessEnv(env)\n",
    "from maze import Maze\n",
    "\n",
    "# Constants\n",
    "GAME_HEIGHT = 600\n",
    "GAME_WIDTH = 600\n",
    "NUMBER_OF_TILES = 25\n",
    "SCREEN_HEIGHT = 700\n",
    "SCREEN_WIDTH = 700\n",
    "TILE_SIZE = GAME_HEIGHT // NUMBER_OF_TILES\n",
    "\n",
    "# Maze layout\n",
    "level = [\n",
    "    \"XXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "    \"X XXXXXXXX          XXXXX\",\n",
    "    \"X XXXXXXXX  XXXXXX  XXXXX\",\n",
    "    \"XP     XXX  XXXXXX  XXXXX\",\n",
    "    \"X      XXX  XXX         X\",\n",
    "    \"XXXXXX  XX  XXX        XX\",\n",
    "    \"XXXXXX  XX  XXXXXX  XXXXX\",\n",
    "    \"XXXXXX  XX  XXXXXX  XXXXX\",\n",
    "    \"X  XXX      XXXXXXXXXXXXX\",\n",
    "    \"X  XXX  XXXXXXXXXXXXXXXXX\",\n",
    "    \"X         XXXXXXXXXXXXXXX\",\n",
    "    \"X             XXXXXXXXXXX\",\n",
    "    \"XXXXXXXXXXX      XXXXX  X\",\n",
    "    \"XXXXXXXXXXXXXXX  XXXXX  X\",\n",
    "    \"XXX  XXXXXXXXXX         X\",\n",
    "    \"XXX                     X\",\n",
    "    \"XXX         XXXXXXXXXXXXX\",\n",
    "    \"XXXXXXXXXX  XXXXXXXXXXXXX\",\n",
    "    \"XXXXXXXXXX              X\",\n",
    "    \"XX   XXXXX              X\",\n",
    "    \"XX   XXXXXXXXXXXXX  XXXXX\",\n",
    "    \"XX    XXXXXXXXXXXX  XXXXX\",\n",
    "    \"XX        XXXX          X\",\n",
    "    \"XXXX                    X\",\n",
    "    \"XXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "]\n",
    "\n",
    "env = Maze(\n",
    "    level,\n",
    "    goal_pos=(23, 20),\n",
    "    MAZE_HEIGHT=GAME_HEIGHT,\n",
    "    MAZE_WIDTH=GAME_WIDTH,\n",
    "    SIZE=NUMBER_OF_TILES,\n",
    ")\n",
    "env = PreProcessEnv(env)\n",
    "NO_OF_ACTIONS = 4\n",
    "\n",
    "# env.unwrapped.start()\n",
    "state = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next State: tensor([[4., 1.]]), Reward: tensor([[-1.]]), Done: tensor([[False]]), Info: {}\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, info = env.step(torch.randint(4, (1,1)))\n",
    "\n",
    "print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}, Info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRLModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier= torch.nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DeepRLModel                              [3, 4]                    --\n",
       "├─Sequential: 1-1                        [3, 4]                    --\n",
       "│    └─Linear: 2-1                       [3, 128]                  384\n",
       "│    └─ReLU: 2-2                         [3, 128]                  --\n",
       "│    └─Linear: 2-3                       [3, 64]                   8,256\n",
       "│    └─ReLU: 2-4                         [3, 64]                   --\n",
       "│    └─Linear: 2-5                       [3, 4]                    260\n",
       "==========================================================================================\n",
       "Total params: 8,900\n",
       "Trainable params: 8,900\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 0.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "q_network = DeepRLModel().to(device)\n",
    "summary(q_network, (3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepRLModel(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_q_network = copy.deepcopy(q_network).to(device)\n",
    "target_q_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2914, -0.2722,  0.2045, -0.1695]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network(next_state.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, epsilon = 0.05):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        return torch.randint(4, (1, 1))\n",
    "    else:\n",
    "        av = q_network(state.to(device))\n",
    "        return torch.argmax(av, dim = -1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from memory import ReplayMemory\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def deep_sarsa(q_network:DeepRLModel, policy, episodes, alpha=0.001, batch_size=32, gamma=0.99, epsilon=0.2):\n",
    "    optim = AdamW(q_network.parameters(), lr=alpha)\n",
    "    memory = ReplayMemory(capacity = 1000000)\n",
    "    stats = {'MSE Loss': [], 'Returns': []}\n",
    "    \n",
    "    for episode in tqdm(range(1, episodes + 1)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0\n",
    "        while not done:\n",
    "            action = policy(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # next_state = next_state.to(device)\n",
    "            # reward = reward.to(device)\n",
    "            # done = done.to(device)\n",
    "            # memory.insert([state, action, reward, done, next_state])\n",
    "            \n",
    "            # if memory.can_sample(batch_size):\n",
    "            #     state_b, action_b, reward_b, done_b, next_state_b = memory.sample(batch_size)\n",
    "\n",
    "            #     state_b = state_b.to(device)\n",
    "            #     reward_b = reward_b.to(device)\n",
    "            #     done_b = done_b.to(device)\n",
    "            #     next_state_b = next_state_b.to(device)\n",
    "            #     action_b = action_b.to(device)\n",
    "                          \n",
    "            #     qsa_b = q_network(state_b).gather(1, action_b).to(device)\n",
    "\n",
    "            #     next_action_b = policy(next_state_b).to(device)\n",
    "            #     next_qsa_b = target_q_network(next_state_b).gather(1, next_action_b).to(device)\n",
    "            #     target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
    "\n",
    "                \n",
    "            #     loss = F.mse_loss(qsa_b, target_b.to(device))\n",
    "            #     q_network.zero_grad()\n",
    "            #     loss.backward()\n",
    "            #     optim.step()\n",
    "                \n",
    "            #     loss.item()\n",
    "            #     stats['MSE Loss'].append(loss.item())\n",
    "            \n",
    "            state = next_state\n",
    "            ep_return += reward.item()\n",
    "        \n",
    "        stats['Returns'].append(ep_return)\n",
    "        if episode % 100 == 0:\n",
    "            target_q_network.load_state_dict(q_network.state_dict())\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Projects\\Reinforcement Learning\\snake\\maze_deep_sarsa.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stats \u001b[39m=\u001b[39m deep_sarsa(q_network, policy, episodes  \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32md:\\Documents\\Projects\\Reinforcement Learning\\snake\\maze_deep_sarsa.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ep_return \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     action \u001b[39m=\u001b[39m policy(state, epsilon)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# next_state = next_state.to(device)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# reward = reward.to(device)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# done = done.to(device)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m#     loss.item()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m#     stats['MSE Loss'].append(loss.item())\u001b[39;00m\n",
      "\u001b[1;32md:\\Documents\\Projects\\Reinforcement Learning\\snake\\maze_deep_sarsa.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mrandint(\u001b[39m4\u001b[39m, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     av \u001b[39m=\u001b[39m q_network(state\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39margmax(av, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\Documents\\Projects\\Reinforcement Learning\\snake\\maze_deep_sarsa.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/Reinforcement%20Learning/snake/maze_deep_sarsa.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\pytorch-nogpu\\Lib\\site-packages\\torch\\nn\\functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1470\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1471\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1472\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = deep_sarsa(q_network, policy, episodes  = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autodrive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
